{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Dataset\n",
    "\n",
    "This notebook filters the data down to N=1000 instances, by selecting 420 questions and 580 statements at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script filters a dataset of ATC (Air Traffic Control) communications\n",
    "# based on word count and identifies potential questions and statements.\n",
    "def filter_atc_dataset(csv_path, output_csv_path, min_words=5):\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    \n",
    "    print(f\"Original dataset size: {len(df)} clips\")\n",
    "    \n",
    "    # Add a column for word count\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Filter by word count\n",
    "    df_filtered = df[df['word_count'] >= min_words]\n",
    "    print(f\"After word count filtering: {len(df_filtered)} clips\")\n",
    "    \n",
    "    # Identify potential questions (still requires manual review)\n",
    "    question_patterns = [\n",
    "        r'\\?',                    # Question mark\n",
    "        r'\\b(?:what|where|when|why|who|how|which)\\b',  # WH-questions\n",
    "        r'\\b(?:do|does|did|is|are|was|were|have|has|had|can|could|will|would|should|may|might)\\s+(?:\\w+\\s+)+\\?',  # Yes/no questions\n",
    "        r'\\b(?:right|correct|copy|roger)\\?',  # Common ATC confirmation questions\n",
    "        r'say again',             # Common in ATC for clarification\n",
    "        r'request',               # Often indicates a question in ATC context\n",
    "        r'confirm',               # Confirmation requests\n",
    "    ]\n",
    "    \n",
    "    # Create a combined pattern for detecting questions\n",
    "    combined_pattern = '|'.join(question_patterns)\n",
    "    \n",
    "    # Mark potential questions\n",
    "    df_filtered['potential_question'] = df_filtered['text'].apply(\n",
    "        lambda x: bool(re.search(combined_pattern, str(x).lower())) if pd.notna(x) else False\n",
    "    )\n",
    "    \n",
    "    # Mark potential statements \n",
    "    df_filtered['potential_statement'] = ~df_filtered['potential_question']\n",
    "    \n",
    "    # Save filtered dataset\n",
    "    df_filtered.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Potential questions: {sum(df_filtered['potential_question'])}\")\n",
    "    print(f\"Potential statements: {sum(df_filtered['potential_statement'])}\")\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with actual paths\n",
    "csv_path = \"train_data.csv\"\n",
    "output_path = \"filtered_train_data.csv\"\n",
    "\n",
    "filtered_df = filter_atc_dataset(\n",
    "    csv_path, \n",
    "    output_path,\n",
    "    min_words=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('filtered_train_data.csv')\n",
    "\n",
    "questions = df[df['potential_question']]\n",
    "\n",
    "statements = df[df['potential_statement']]\n",
    "\n",
    "\n",
    "# Randomly sample 580 statements (total rows = 1000)\n",
    "statements_sampled = statements.sample(n=580, random_state=23)  # Set random_state for reproducibility\n",
    "\n",
    "# Concatenate back the questions and the sampled statements\n",
    "df_filtered = pd.concat([questions, statements_sampled])\n",
    "\n",
    "# Shuffle the final dataframe (optional)\n",
    "df_filtered = df_filtered.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# add blank label column for now\n",
    "df_filtered['label'] = None\n",
    "\n",
    "\n",
    "\n",
    "print(df_filtered)\n",
    "\n",
    "# Save the final filtered dataset of length 1000\n",
    "df_filtered.to_csv('N_1000_filtered_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
